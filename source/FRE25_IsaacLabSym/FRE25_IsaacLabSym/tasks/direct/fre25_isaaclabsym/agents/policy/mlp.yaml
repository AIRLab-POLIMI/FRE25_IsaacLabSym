# @package _global_
# MLP Policy Configuration
# Fast, feedforward network with optional hidden states for memory
# Hidden states: policy outputs dummy actions that don't control robot
# ALL past actions (control + hidden) are fed back as observations
# 
# Control actions:
#   - steering: agent controls left/right turning
#   - throttle: agent controls forward/backward speed
#   - step_command: agent decides when to advance command buffer (NEW!)
# 
# Usage: ./RUN_SB3_TRAIN.sh policy=mlp

# Metadata for algorithm selection
policy_type: "mlp"
algorithm: "PPO"
policy_class: "MlpPolicy"

# Hidden states configuration
# Policy outputs: [steering, throttle, step_command, hidden1, hidden2, ...]
# ALL past actions are fed back as observations to provide temporal memory
# Set to 0 to disable, or positive integer for number of hidden state dimensions
num_hidden_states: 8 # Options: 0 (none), 2, 4, 8, 16, etc.

# MLP-optimized training parameters
n_steps: 2048      # Larger batches for better GPU utilization
batch_size: 2048   # Can process more in parallel

# MLP network architecture
policy_kwargs:
  net_arch:
    - 512
    - 256
    - 128
  ortho_init: true
