# Stable-Baselines3 PPO Configuration for FRE25 with DISCRETE Actions
# Uses MultiDiscrete action space: 2 actions (steering, wheels), each with 3 categories {-1, 0, 1}
# 
# Supports both MLP and LSTM policies via Hydra composition
# Switch with: ./RUN_SB3_TRAIN.sh policy=mlp  OR  policy=lstm

# Hydra composition - specify which policy to use
# NOTE: Commenting out defaults for now as IsaacLab's Hydra integration doesn't support config groups well
# defaults:
#   - policy: mlp  # Options: mlp (default, fast) or lstm (temporal memory)

seed: 42

# Policy configuration - MLP by default
policy_type: mlp  # Options: mlp or lstm
algorithm: PPO  # PPO for MLP, RecurrentPPO for LSTM
policy_class: MlpPolicy  # MlpPolicy for MLP, MlpLstmPolicy for LSTM

# Hidden accumulator configuration (MLP only)
# Set to 0 to disable, or positive integer for number of accumulator dimensions
num_hidden_accumulators: 8  # Default: 8 accumulators for MLP (set to 0 to disable)

# Training parameters (can be overridden by policy-specific configs)
n_timesteps: 10000000000000000  # Total timesteps for training
n_steps: 2048  # Steps per env per update (overridden by policy configs)
batch_size: 2048  # Minibatch size (overridden by policy configs)
n_epochs: 10  # Number of epochs when optimizing the surrogate loss

# PPO hyperparameters
learning_rate: 0.0003  # Learning rate
gamma: 0.996  # Discount factor (horizon ~250 steps)
gae_lambda: 0.95  # Factor for trade-off of bias vs variance for GAE
clip_range: 0.2  # Clipping parameter for PPO
ent_coef: 0.01  # Entropy coefficient for the loss calculation (higher for discrete actions)
vf_coef: 2.0  # Value function coefficient for the loss calculation
max_grad_norm: 1.0  # Max value for gradient clipping

# Base policy architecture (overridden by policy-specific configs)
policy_kwargs:
  ortho_init: true  # Use orthogonal initialization

# Normalization
normalize_input: false  # Normalize observations
normalize_value: false  # Normalize rewards
clip_obs: 10.0  # Clip observations
