# Stable-Baselines3 PPO Configuration for FRE25 with DISCRETE Actions
# Uses MultiDiscrete action space: 3 control actions + N hidden actions
# Control actions:
#   - steering: 3 categories {-1, 0, 1}
#   - throttle: 3 categories {-1, 0, 1}
#   - step_command: 2 categories {0, 1} - agent decides when to advance command buffer
# Hidden actions: dummy actions that don't affect robot, used for policy memory
# 
# ALL past actions (control + hidden) are fed back as observations
# 
# Supports both MLP and LSTM policies via Hydra composition
# Switch with: ./RUN_SB3_TRAIN.sh policy=mlp  OR  policy=lstm

# Hydra composition - specify which policy to use
# NOTE: Commenting out defaults for now as IsaacLab's Hydra integration doesn't support config groups well
# defaults:
#   - policy: mlp  # Options: mlp (default, fast) or lstm (temporal memory)

seed: 42

# Policy configuration - MLP by default
policy_type: mlp  # Options: mlp or lstm
algorithm: PPO  # PPO for MLP, RecurrentPPO for LSTM
policy_class: MlpPolicy  # MlpPolicy for MLP, MlpLstmPolicy for LSTM

# Hidden states configuration
# Hidden actions are "dummy" outputs that don't control the robot
# The policy outputs: [steering, throttle, step_command, hidden1, hidden2, ...]
# ALL past actions are fed back as observations to provide memory
# Example: num_hidden_states=2 â†’ 5 total actions (3 control + 2 hidden)
num_hidden_states: 8  # Default: 8 hidden state dimensions (set to 0 to disable)

# Training parameters (can be overridden by policy-specific configs)
n_timesteps: 10000000000000000  # Total timesteps for training
n_steps: 2048  # Steps per env per update (overridden by policy configs)
batch_size: 2048  # Minibatch size (overridden by policy configs)
n_epochs: 10  # Number of epochs when optimizing the surrogate loss

# PPO hyperparameters
learning_rate: 0.0003  # Learning rate
gamma: 0.996  # Discount factor (horizon ~250 steps)
gae_lambda: 0.95  # Factor for trade-off of bias vs variance for GAE
clip_range: 0.2  # Clipping parameter for PPO
ent_coef: 0.01  # Entropy coefficient for the loss calculation (higher for discrete actions)
vf_coef: 2.0  # Value function coefficient for the loss calculation
max_grad_norm: 1.0  # Max value for gradient clipping

# Base policy architecture (overridden by policy-specific configs)
policy_kwargs:
  ortho_init: true  # Use orthogonal initialization

# Normalization
normalize_input: false  # Normalize observations
normalize_value: false  # Normalize rewards
clip_obs: 10.0  # Clip observations
